{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMAGE_SIZE = 150\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Set up data augmentation and normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Splitting data into training and validation sets\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 696 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'Chest/Train',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    classes=['Normal', 'PNEUMONIA']\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'Chest/Test',\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    classes=['Normal', 'PNEUMONIA']  # Explicitly specify class labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(1, activation='sigmoid')\n",
    "], name='PNEUMONIA_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "163/163 [==============================] - 261s 2s/step - loss: 0.3091 - accuracy: 0.8882 - val_loss: 2.3901 - val_accuracy: 0.6161\n",
      "Epoch 2/10\n",
      "163/163 [==============================] - 211s 1s/step - loss: 0.2294 - accuracy: 0.9151 - val_loss: 3.0889 - val_accuracy: 0.6161\n",
      "Epoch 3/10\n",
      "163/163 [==============================] - 190s 1s/step - loss: 0.2081 - accuracy: 0.9191 - val_loss: 2.2704 - val_accuracy: 0.6131\n",
      "Epoch 4/10\n",
      "163/163 [==============================] - 187s 1s/step - loss: 0.1858 - accuracy: 0.9289 - val_loss: 0.5053 - val_accuracy: 0.7649\n",
      "Epoch 5/10\n",
      "163/163 [==============================] - 188s 1s/step - loss: 0.1794 - accuracy: 0.9293 - val_loss: 0.5270 - val_accuracy: 0.8095\n",
      "Epoch 6/10\n",
      "163/163 [==============================] - 185s 1s/step - loss: 0.1606 - accuracy: 0.9404 - val_loss: 1.3661 - val_accuracy: 0.6518\n",
      "Epoch 7/10\n",
      "163/163 [==============================] - 185s 1s/step - loss: 0.1457 - accuracy: 0.9450 - val_loss: 1.5239 - val_accuracy: 0.5521\n",
      "Epoch 8/10\n",
      "163/163 [==============================] - 188s 1s/step - loss: 0.1297 - accuracy: 0.9509 - val_loss: 4.2494 - val_accuracy: 0.6146\n",
      "Epoch 9/10\n",
      "163/163 [==============================] - 186s 1s/step - loss: 0.1439 - accuracy: 0.9482 - val_loss: 0.6417 - val_accuracy: 0.7946\n",
      "Epoch 10/10\n",
      "163/163 [==============================] - 184s 1s/step - loss: 0.1335 - accuracy: 0.9534 - val_loss: 1.4804 - val_accuracy: 0.6473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a1825d0d10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the generators\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brigh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('PNEUMONIA_classifier_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG 16 model\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load the VGG16 model\n",
    "vgg16_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "# Freeze all the layers\n",
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Create a new model on top of the pre-trained base model\n",
    "model2 = Sequential([\n",
    "    vgg16_model,\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 8s 8s/step - loss: 0.9252 - accuracy: 0.3750 - val_loss: 1.5498 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.6847 - accuracy: 0.7500 - val_loss: 0.8771 - val_accuracy: 0.6094\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.3082 - accuracy: 0.8125 - val_loss: 0.1977 - val_accuracy: 0.9062\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.1021 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.1018 - accuracy: 1.0000 - val_loss: 0.1086 - val_accuracy: 0.9531\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 7s 7s/step - loss: 0.0737 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9531\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 0.0715 - val_accuracy: 0.9688\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 6s 6s/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2303dc90c90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the generators\n",
    "model2.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brigh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model2.save('tomato_classifier_VGG16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 625ms/step\n",
      "prediction of model 1\n",
      "The tomato is classified as 'PNEUMONIA'.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you've trained and saved your model already...\n",
    "\n",
    "# Load the saved models\n",
    "model = tf.keras.models.load_model('PNEUMONIA_classifier_model.h5')\n",
    "# loaded_model = tf.keras.models.load_model('tomato_classifier_VGG16.h5')\n",
    "\n",
    "# Now, assuming you have a new image you want to classify\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load and preprocess the new image\n",
    "new_image_path = 'Chest/Test/PNEUMONIA/0111.jpeg'  # Replace with the path to your new image\n",
    "new_img = image.load_img(new_image_path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "new_img_array = image.img_to_array(new_img)\n",
    "new_img_array = np.expand_dims(new_img_array, axis=0)\n",
    "new_img_array /= 255.  # Normalize the image\n",
    "\n",
    "# Make a prediction using the loaded model\n",
    "prediction = model.predict(new_img_array)\n",
    "# prediction1 = loaded_model.predict(new_img_array)\n",
    "\n",
    "# Interpret the prediction\n",
    "print(\"prediction of model 1\")\n",
    "if prediction[0] < 0.5:\n",
    "    print(\"The tomato is classified as 'Normal'.\")\n",
    "else:\n",
    "    print(\"The tomato is classified as 'PNEUMONIA'.\")\n",
    "\n",
    "# print(\"prediction of model vgg16\")\n",
    "# if prediction1[0] < 0.5:\n",
    "#     print(\"The tomato is classified as 'good'.\")\n",
    "# else:\n",
    "#     print(\"The tomato is classified as 'bad'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define constants\n",
    "IMAGE_SIZE = 150  # Adjust this according to your image size\n",
    "dataset_path = 'Chest'  # Replace this with your dataset path\n",
    "csv_file_path = 'chest_dataset_with_images.csv'\n",
    "excel_file_path = 'chest_dataset.xlsx'\n",
    "\n",
    "# Open CSV file in write mode\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Filename', 'Label', 'Image', 'Set'])  # Write header\n",
    "\n",
    "    # Iterate through Train and Test folders\n",
    "    for folder in ['Train', 'Test']:\n",
    "        for class_name in ['Normal', 'PNEUMONIA']:\n",
    "            class_dir = os.path.join(dataset_path, folder, class_name)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                file_path = os.path.join(class_dir, filename)\n",
    "\n",
    "                # Check if the file is an image by attempting to open it with PIL\n",
    "                try:\n",
    "                    img = Image.open(file_path)\n",
    "                    img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "                except (IOError, OSError):\n",
    "                    continue  # Skip if it's not an image file\n",
    "\n",
    "                # Convert image to numpy array\n",
    "                img_array = np.array(img)\n",
    "                \n",
    "                # Convert image array to base64 string\n",
    "                img_str = img_array.tobytes()\n",
    "                img_base64 = base64.b64encode(img_str).decode('utf-8')\n",
    "\n",
    "                # Determine if the image is in the 'Train' or 'Test' set\n",
    "                image_set = folder\n",
    "\n",
    "                writer.writerow([file_path, class_name, img_base64, image_set])\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "data.to_excel(excel_file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
